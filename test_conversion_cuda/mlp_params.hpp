#ifndef MLP_PARAMS_HPP
#define MLP_PARAMS_HPP

// Auto-generated by write_mlp_params_split.py
// Source Checkpoint: NN_wm_CH1_G0_S1_TBL1_tn543759_vn135940_fds0_lds0_customw1_inputs2_final_ep8000_tl0.03098575_vl0.03911465.pth
// Model structure: Linear(3->40)+ReLU -> 3x[Linear(40->40)+ReLU] -> Linear(40->1)

// #include <cti_utils_gpu.hpp> // Assuming this defines cti_ffp
// Or define cti_ffp if not included elsewhere:
using cti_ffp = float; 

// --- Network Architecture Constants ---
const int Nlayers_wall = 5;
const int Nneurons_wall = 40;
const int act_fn_wall = 0; // 0 assumed for ReLU

// --- Extern Declarations for Weights, Biases, and Parameters ---
// NOTE: Array sizes corrected based on model architecture

extern const cti_ffp input_layer_wall[120];      // Input weights (3x40)
extern const cti_ffp hidden_layers_wall[4800];  // Hidden weights (3x 40x40)
extern const cti_ffp bias_wall[161];           // All biases (40+40+40+40+1)
extern const cti_ffp output_layer_wall[41];   // Output weights + bias (40+1)

#endif // MLP_PARAMS_HPP
